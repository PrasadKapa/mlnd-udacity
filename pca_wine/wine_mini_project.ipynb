{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic_Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Ash_Alcanity</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total_Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid_Phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color_Intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Customer_Segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.20</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.45</td>\n",
       "      <td>15.2</td>\n",
       "      <td>112</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1450</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.39</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.45</td>\n",
       "      <td>14.6</td>\n",
       "      <td>96</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1.02</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1290</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.06</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.61</td>\n",
       "      <td>17.6</td>\n",
       "      <td>121</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.05</td>\n",
       "      <td>1.06</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1295</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.83</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1045</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13.86</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.27</td>\n",
       "      <td>16.0</td>\n",
       "      <td>98</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.85</td>\n",
       "      <td>7.22</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1045</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol  Malic_Acid   Ash  Ash_Alcanity  Magnesium  Total_Phenols  \\\n",
       "0    14.23        1.71  2.43          15.6        127           2.80   \n",
       "1    13.20        1.78  2.14          11.2        100           2.65   \n",
       "2    13.16        2.36  2.67          18.6        101           2.80   \n",
       "3    14.37        1.95  2.50          16.8        113           3.85   \n",
       "4    13.24        2.59  2.87          21.0        118           2.80   \n",
       "5    14.20        1.76  2.45          15.2        112           3.27   \n",
       "6    14.39        1.87  2.45          14.6         96           2.50   \n",
       "7    14.06        2.15  2.61          17.6        121           2.60   \n",
       "8    14.83        1.64  2.17          14.0         97           2.80   \n",
       "9    13.86        1.35  2.27          16.0         98           2.98   \n",
       "\n",
       "   Flavanoids  Nonflavanoid_Phenols  Proanthocyanins  Color_Intensity   Hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "5        3.39                  0.34             1.97             6.75  1.05   \n",
       "6        2.52                  0.30             1.98             5.25  1.02   \n",
       "7        2.51                  0.31             1.25             5.05  1.06   \n",
       "8        2.98                  0.29             1.98             5.20  1.08   \n",
       "9        3.15                  0.22             1.85             7.22  1.01   \n",
       "\n",
       "   OD280  Proline  Customer_Segment  \n",
       "0   3.92     1065                 1  \n",
       "1   3.40     1050                 1  \n",
       "2   3.17     1185                 1  \n",
       "3   3.45     1480                 1  \n",
       "4   2.93      735                 1  \n",
       "5   2.85     1450                 1  \n",
       "6   3.58     1290                 1  \n",
       "7   3.58     1295                 1  \n",
       "8   2.85     1045                 1  \n",
       "9   3.55     1045                 1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Wine.csv')\n",
    "X = dataset.iloc[:, 0:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature Scaling must be applied when reducing dimensionality\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.36884109,  0.19318394,  0.10752862,  0.07421996,  0.06245904,\n",
       "        0.04909   ,  0.04117287,  0.02495984,  0.02308855,  0.01864124,\n",
       "        0.01731766,  0.01252785,  0.00696933])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying PCA\n",
    "from sklearn.decomposition import PCA\n",
    "# The number of extracted features you want to get\n",
    "# Start with None and get the explained_variance\n",
    "pca = PCA(n_components = None) \n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "# Note that you get 13 PCs sorted by the most variance \n",
    "# Now you can go up and update the number of n_components as you wish\n",
    "explained_variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.36884109,  0.19318394,  0.10752862])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying PCA with n_components = 2\n",
    "from sklearn.decomposition import PCA\n",
    "# The number of extracted features you want to get\n",
    "# Start with None and get the explained_variance\n",
    "pca = PCA(n_components = 3) \n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "explained_variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.17884511e+00,  -1.07218467e+00],\n",
       "       [ -1.80819239e+00,   1.57822344e+00],\n",
       "       [  1.09829474e+00,   2.22124345e+00],\n",
       "       [ -2.55584748e+00,  -1.66210369e+00],\n",
       "       [  1.85698063e+00,   2.41573166e-01],\n",
       "       [  2.58288576e+00,  -1.37668170e+00],\n",
       "       [  8.72876119e-01,   2.25618512e+00],\n",
       "       [ -4.18384029e-01,   2.35415681e+00],\n",
       "       [ -3.04977245e-01,   2.27659433e+00],\n",
       "       [  2.14082532e+00,  -1.10052871e+00],\n",
       "       [ -2.98136465e+00,  -2.47159183e-01],\n",
       "       [  1.96188242e+00,   1.25407738e+00],\n",
       "       [ -2.16177795e+00,  -9.75966550e-01],\n",
       "       [  2.21976084e+00,  -2.39507167e+00],\n",
       "       [ -2.30179529e+00,  -2.05791962e-01],\n",
       "       [ -3.00953827e+00,  -2.79141212e-01],\n",
       "       [  2.63443473e+00,  -8.68313119e-01],\n",
       "       [ -1.09219965e+00,  -3.53906625e+00],\n",
       "       [  2.62578435e+00,  -2.96852840e-03],\n",
       "       [  1.98077342e-01,   2.29193443e+00],\n",
       "       [ -2.67442753e+00,  -2.58800132e+00],\n",
       "       [ -2.54763698e+00,  -4.52703891e-01],\n",
       "       [  1.77416736e+00,   8.43586940e-01],\n",
       "       [ -2.77786938e+00,  -4.32090258e-01],\n",
       "       [ -2.86679938e+00,  -1.87580875e+00],\n",
       "       [  1.35498845e+00,   3.99545184e-02],\n",
       "       [ -2.43900474e+00,   9.44074889e-02],\n",
       "       [ -2.27268121e+00,   5.05883053e-01],\n",
       "       [  1.17887166e+00,   2.50068415e+00],\n",
       "       [ -2.30673313e+00,   1.30502777e+00],\n",
       "       [ -2.53020738e+00,  -5.51277126e-01],\n",
       "       [  1.63200028e-01,   1.29107817e+00],\n",
       "       [  2.57881158e+00,  -1.17515982e+00],\n",
       "       [ -7.62471566e-01,   3.16097049e+00],\n",
       "       [  2.57005937e+00,  -9.66718786e-02],\n",
       "       [ -4.78337042e-01,   5.77763823e-01],\n",
       "       [  5.47417096e-01,  -3.77647780e-01],\n",
       "       [  3.55763538e+00,  -1.45816125e+00],\n",
       "       [  1.69260971e+00,   1.37844174e+00],\n",
       "       [  2.65288395e+00,  -2.39399539e-01],\n",
       "       [ -3.62047411e+00,  -6.90153979e-01],\n",
       "       [ -1.61462317e+00,  -2.41170340e+00],\n",
       "       [  1.50959767e+00,   1.32717326e+00],\n",
       "       [  5.36413494e-02,   2.07680094e+00],\n",
       "       [ -1.07889168e-01,   2.85115217e+00],\n",
       "       [ -2.39610454e+00,  -2.45883860e+00],\n",
       "       [ -3.12315181e+00,   4.23261512e-01],\n",
       "       [  3.28569649e+00,  -3.22859884e-01],\n",
       "       [ -3.55506872e+00,  -1.74242946e+00],\n",
       "       [ -3.87020538e-01,   2.61510101e+00],\n",
       "       [ -4.74514016e-01,   1.98023790e+00],\n",
       "       [ -1.06865761e+00,   6.78906271e-01],\n",
       "       [  1.08546036e+00,   1.30817801e+00],\n",
       "       [  2.02340107e+00,   1.56926094e+00],\n",
       "       [  2.76257094e+00,  -1.85603600e+00],\n",
       "       [  2.06778286e+00,  -1.35861191e+00],\n",
       "       [  9.06600653e-01,   2.04110996e+00],\n",
       "       [  3.52336455e+00,  -1.39946872e+00],\n",
       "       [ -3.79914321e+00,  -4.92727617e-02],\n",
       "       [  1.75250993e+00,   4.64283651e-01],\n",
       "       [ -3.46539192e+00,  -7.90385134e-01],\n",
       "       [  3.16221605e+00,  -8.39879111e-01],\n",
       "       [  2.37656864e+00,  -1.72475988e+00],\n",
       "       [  1.31278073e+00,   8.53348760e-01],\n",
       "       [  3.57258440e+00,  -1.78091597e+00],\n",
       "       [  9.50877158e-01,   2.38927332e+00],\n",
       "       [  4.95461316e-01,   2.16498322e+00],\n",
       "       [  3.79294638e+00,  -2.92787186e+00],\n",
       "       [ -2.37978591e+00,  -2.13572422e+00],\n",
       "       [ -1.50346992e+00,   1.39146991e+00],\n",
       "       [  2.50566646e+00,  -1.30365941e+00],\n",
       "       [ -6.97535788e-01,   2.78160736e-01],\n",
       "       [ -7.25562555e-01,   2.54007170e+00],\n",
       "       [  9.24047324e-01,   1.46344718e+00],\n",
       "       [ -1.25151294e+00,   2.74792621e-02],\n",
       "       [  2.20937835e+00,  -8.05690832e-01],\n",
       "       [ -3.84416995e+00,  -5.74263508e-01],\n",
       "       [ -1.77983157e+00,  -1.38012167e+00],\n",
       "       [  4.35413058e+00,  -2.33750318e+00],\n",
       "       [  3.33834347e+00,  -1.51169086e+00],\n",
       "       [ -1.44769123e+00,   1.90826204e+00],\n",
       "       [ -2.75066706e+00,  -2.07100640e+00],\n",
       "       [  2.79475799e+00,  -1.36659228e+00],\n",
       "       [  1.84642601e+00,  -6.82481476e-01],\n",
       "       [ -4.13332842e-01,   2.20440158e+00],\n",
       "       [ -4.81356617e-02,   1.17469609e+00],\n",
       "       [  1.99166500e+00,  -2.50860656e-01],\n",
       "       [  2.26421169e+00,  -1.32120813e+00],\n",
       "       [  7.85551414e-01,  -2.46487051e-01],\n",
       "       [ -3.32586984e+00,  -2.14485564e+00],\n",
       "       [  1.00496881e+00,   7.20390295e-01],\n",
       "       [  2.31479633e+00,   2.62129546e-01],\n",
       "       [  8.67032066e-01,   1.36440259e+00],\n",
       "       [ -2.28629769e+00,  -4.54244754e-01],\n",
       "       [  3.14452871e+00,  -1.29318898e+00],\n",
       "       [ -1.65297942e+00,   1.74177394e+00],\n",
       "       [ -2.84689388e+00,  -7.78426712e-02],\n",
       "       [ -2.68393126e+00,  -2.53813173e-01],\n",
       "       [  1.97280128e+00,  -1.70171835e+00],\n",
       "       [  1.63120111e+00,   7.24762688e-01],\n",
       "       [ -2.05082836e+00,  -2.11848206e-01],\n",
       "       [  2.28798382e+00,  -1.95899701e+00],\n",
       "       [ -2.28266458e+00,  -2.07243579e-01],\n",
       "       [ -3.45079842e-01,   1.92360626e+00],\n",
       "       [  1.49448758e+00,  -7.18673825e-01],\n",
       "       [  2.26695932e+00,  -7.93531817e-01],\n",
       "       [ -3.44673144e-01,   1.92686997e+00],\n",
       "       [  2.75927029e+00,  -1.56391999e+00],\n",
       "       [ -2.86839562e+00,  -1.85579453e+00],\n",
       "       [ -1.46228982e+00,   1.22151405e+00],\n",
       "       [ -3.31754434e+00,  -1.20382601e+00],\n",
       "       [  1.72057718e+00,  -1.36843828e-01],\n",
       "       [ -2.90065973e+00,  -3.71521776e-01],\n",
       "       [ -2.30532411e+00,  -2.14386284e+00],\n",
       "       [ -3.51377495e+00,  -1.17981731e+00],\n",
       "       [  2.32780065e+00,  -2.95396131e-01],\n",
       "       [  1.54528723e+00,   1.99996309e+00],\n",
       "       [ -3.32660657e-01,   2.37118865e+00],\n",
       "       [  4.96648201e-01,   9.57282660e-01],\n",
       "       [  6.69654741e-01,   3.80907536e+00],\n",
       "       [ -2.76009366e+00,  -1.48785734e+00],\n",
       "       [ -3.19187371e+00,  -2.70815669e+00],\n",
       "       [ -6.05023707e-01,   1.10583182e+00],\n",
       "       [ -1.34006934e+00,   1.51232906e+00],\n",
       "       [  1.05506599e+00,  -9.49724036e-01],\n",
       "       [  1.16017702e+00,   1.39768493e+00],\n",
       "       [ -2.87675356e+00,  -1.15157946e+00],\n",
       "       [ -2.35838421e+00,  -2.44842974e+00],\n",
       "       [  2.54704855e+00,  -1.86824592e+00],\n",
       "       [  3.20597222e+00,  -1.85912926e+00],\n",
       "       [ -2.69949485e+00,  -1.75638262e-01],\n",
       "       [ -9.67436859e-01,   1.81399824e+00],\n",
       "       [ -1.46454259e+00,   1.01680272e+00],\n",
       "       [ -4.04000223e-01,   2.40815711e+00],\n",
       "       [  1.46393837e+00,  -6.90763351e-01],\n",
       "       [  1.15903114e+00,  -2.91379684e-01],\n",
       "       [  2.82057099e+00,  -8.99578955e-01],\n",
       "       [ -5.01011897e-01,   2.68453162e+00],\n",
       "       [  3.30453915e-01,   2.43396193e+00],\n",
       "       [  1.09727608e-02,   1.99585453e+00],\n",
       "       [  2.89176687e+00,  -7.71555485e-01],\n",
       "       [ -2.44830439e+00,  -2.11360296e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the training set. Notice that we now have only two features!\n",
    "# Now we are ready for fit our learning model and visualize it\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14,  0,  0],\n",
       "       [ 0, 16,  0],\n",
       "       [ 0,  0,  6]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (14 + 16 + 6) / 36\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 2 features per sample; expecting 7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fc18f1bd3522>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n\u001b[1;32m      5\u001b[0m                      np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n\u001b[0;32m----> 6\u001b[0;31m plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n\u001b[0m\u001b[1;32m      7\u001b[0m              alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 305\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 2 features per sample; expecting 7"
     ]
    }
   ],
   "source": [
    "# Visualising the Training set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_train, y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\n",
    "plt.title('Logistic Regression (Training set)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCqsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113ae1f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the Test set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\n",
    "plt.title('Logistic Regression (Test set)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
